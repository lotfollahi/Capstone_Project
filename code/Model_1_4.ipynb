{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Conv3D, MaxPooling3D\n",
    "from keras.regularizers import l2,l1,l1_l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = '/home/ec2-user/Capstone_Project/code/left-pca16' # Full path should be given here \n",
    "train_mask_path = 'mask_updated/train/'\n",
    "test_data_path = '/home/ec2-user/Capstone_Project/code/right-pca16' # Full path should be given here \n",
    "test_mask_path = 'mask_updated/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data form training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,Y_train, num_bands = utils.load_data(data_path = train_data_path, \n",
    "                                             masks_path = train_mask_path, \n",
    "                                             crop_size = 33, \n",
    "                                             num_classes = 6, \n",
    "                                             samples = 100000, \n",
    "                                             balance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test,Y_test, num_bands = utils.load_data(data_path = test_data_path, \n",
    "                                           masks_path = test_mask_path, \n",
    "                                           crop_size = 33, \n",
    "                                           num_classes = 6, \n",
    "                                           samples = 100000, \n",
    "                                           balance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creat generato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(599996, 33, 33, 16) (599996, 6) 16\n",
      "(600000, 33, 33, 16) (600000, 6) 16\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), type(Y_train))\n",
    "print(X_train.shape, Y_train.shape, num_bands)\n",
    "print(X_test.shape,  Y_test.shape,  num_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([X_train[i] for i in range(X_train.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array([X_test[i] for i in range(X_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.array([Y_train[i,:] for i in range(Y_train.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = np.array([Y_test[i,:] for i in range(Y_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), strides=(1, 1), input_shape=(33, 33, 16), kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides=(1, 1), kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides=(1, 1), kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filepath=\"Model_1_4_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599999 samples, validate on 600000 samples\n",
      "Epoch 1/20\n",
      "599999/599999 [==============================] - 226s 377us/step - loss: 1.1537 - acc: 0.9431 - val_loss: 0.7934 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85011, saving model to Model_1_4_best.h5\n",
      "Epoch 2/20\n",
      "599999/599999 [==============================] - 289s 481us/step - loss: 0.3404 - acc: 0.9688 - val_loss: 0.8065 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.85011\n",
      "Epoch 3/20\n",
      "599999/599999 [==============================] - 292s 487us/step - loss: 0.2220 - acc: 0.9756 - val_loss: 0.6178 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85011 to 0.85778, saving model to Model_1_4_best.h5\n",
      "Epoch 4/20\n",
      "599999/599999 [==============================] - 281s 468us/step - loss: 0.1729 - acc: 0.9789 - val_loss: 0.6269 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85778\n",
      "Epoch 5/20\n",
      "167296/599999 [=======>......................] - ETA: 2:20 - loss: 0.1534 - acc: 0.9807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599999/599999 [==============================] - 288s 480us/step - loss: 0.1475 - acc: 0.9809 - val_loss: 0.5325 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85778\n",
      "Epoch 6/20\n",
      "599999/599999 [==============================] - 287s 479us/step - loss: 0.1314 - acc: 0.9825 - val_loss: 0.4582 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85778 to 0.87107, saving model to Model_1_4_best.h5\n",
      "Epoch 7/20\n",
      "599999/599999 [==============================] - 292s 487us/step - loss: 0.1206 - acc: 0.9834 - val_loss: 0.7473 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87107\n",
      "Epoch 8/20\n",
      "599999/599999 [==============================] - 291s 486us/step - loss: 0.1123 - acc: 0.9842 - val_loss: 0.5529 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87107\n",
      "Epoch 9/20\n",
      "599999/599999 [==============================] - 292s 486us/step - loss: 0.1058 - acc: 0.9851 - val_loss: 0.8402 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87107\n",
      "Epoch 10/20\n",
      "599999/599999 [==============================] - 291s 486us/step - loss: 0.1005 - acc: 0.9859 - val_loss: 0.9554 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87107\n",
      "Epoch 11/20\n",
      "599999/599999 [==============================] - 291s 485us/step - loss: 0.0964 - acc: 0.9862 - val_loss: 0.8214 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87107\n",
      "Epoch 12/20\n",
      "599999/599999 [==============================] - 279s 465us/step - loss: 0.0931 - acc: 0.9866 - val_loss: 0.5739 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87107\n",
      "Epoch 13/20\n",
      "599999/599999 [==============================] - 291s 485us/step - loss: 0.0902 - acc: 0.9872 - val_loss: 0.6386 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87107\n",
      "Epoch 14/20\n",
      "599999/599999 [==============================] - 291s 485us/step - loss: 0.0868 - acc: 0.9877 - val_loss: 0.4923 - val_acc: 0.8459\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87107\n",
      "Epoch 15/20\n",
      "599999/599999 [==============================] - 292s 487us/step - loss: 0.0849 - acc: 0.9877 - val_loss: 0.7370 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87107\n",
      "Epoch 16/20\n",
      "599999/599999 [==============================] - 291s 486us/step - loss: 0.0823 - acc: 0.9882 - val_loss: 0.4813 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.87107 to 0.87514, saving model to Model_1_4_best.h5\n",
      "Epoch 17/20\n",
      "599936/599999 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9886"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 20,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('model_1_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = load_model('model_1_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # this make aure all the layers are trainable\n",
    "# for layer in model.layers:\n",
    "#     print(layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH Model - Initialize with TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=30)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_tn_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_tn', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/20\n",
      "599995/599995 [==============================] - 240s 400us/step - loss: 1.0005 - acc: 0.9409 - val_loss: 0.7963 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81741, saving model to Model_1_4_tn_best.h5\n",
      "Epoch 2/20\n",
      "599995/599995 [==============================] - 235s 392us/step - loss: 0.2553 - acc: 0.9720 - val_loss: 0.4951 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81741 to 0.87400, saving model to Model_1_4_tn_best.h5\n",
      "Epoch 3/20\n",
      "599995/599995 [==============================] - 236s 393us/step - loss: 0.1819 - acc: 0.9776 - val_loss: 0.4033 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87400 to 0.91013, saving model to Model_1_4_tn_best.h5\n",
      "Epoch 4/20\n",
      "599995/599995 [==============================] - 234s 390us/step - loss: 0.1492 - acc: 0.9805 - val_loss: 0.4713 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91013\n",
      "Epoch 5/20\n",
      "599995/599995 [==============================] - 232s 386us/step - loss: 0.1307 - acc: 0.9823 - val_loss: 0.5171 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91013\n",
      "Epoch 6/20\n",
      "599995/599995 [==============================] - 244s 407us/step - loss: 0.1189 - acc: 0.9833 - val_loss: 0.5890 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91013\n",
      "Epoch 7/20\n",
      "599995/599995 [==============================] - 236s 394us/step - loss: 0.1104 - acc: 0.9845 - val_loss: 0.4296 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91013\n",
      "Epoch 8/20\n",
      "599995/599995 [==============================] - 230s 384us/step - loss: 0.1045 - acc: 0.9851 - val_loss: 0.5065 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91013\n",
      "Epoch 9/20\n",
      "599995/599995 [==============================] - 236s 394us/step - loss: 0.0988 - acc: 0.9860 - val_loss: 0.6500 - val_acc: 0.8357\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91013\n",
      "Epoch 10/20\n",
      "599995/599995 [==============================] - 237s 395us/step - loss: 0.0939 - acc: 0.9866 - val_loss: 0.3629 - val_acc: 0.9039\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91013\n",
      "Epoch 11/20\n",
      "599995/599995 [==============================] - 239s 398us/step - loss: 0.0905 - acc: 0.9871 - val_loss: 0.6790 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91013\n",
      "Epoch 12/20\n",
      "599995/599995 [==============================] - 235s 392us/step - loss: 0.0876 - acc: 0.9875 - val_loss: 0.6334 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91013\n",
      "Epoch 13/20\n",
      "599995/599995 [==============================] - 229s 382us/step - loss: 0.0847 - acc: 0.9878 - val_loss: 0.4790 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91013\n",
      "Epoch 14/20\n",
      "599995/599995 [==============================] - 229s 382us/step - loss: 0.0827 - acc: 0.9881 - val_loss: 0.4356 - val_acc: 0.8994\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91013\n",
      "Epoch 15/20\n",
      "599995/599995 [==============================] - 230s 384us/step - loss: 0.0805 - acc: 0.9885 - val_loss: 0.5042 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.91013\n",
      "Epoch 16/20\n",
      "599995/599995 [==============================] - 246s 411us/step - loss: 0.0785 - acc: 0.9888 - val_loss: 1.1126 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.91013\n",
      "Epoch 17/20\n",
      "599995/599995 [==============================] - 245s 409us/step - loss: 0.0770 - acc: 0.9891 - val_loss: 0.5145 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.91013\n",
      "Epoch 18/20\n",
      "599995/599995 [==============================] - 240s 400us/step - loss: 0.0752 - acc: 0.9893 - val_loss: 0.4193 - val_acc: 0.8869\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.91013\n",
      "Epoch 19/20\n",
      "599995/599995 [==============================] - 246s 410us/step - loss: 0.0739 - acc: 0.9895 - val_loss: 0.8193 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.91013\n",
      "Epoch 20/20\n",
      "599995/599995 [==============================] - 246s 409us/step - loss: 0.0724 - acc: 0.9897 - val_loss: 0.4769 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.91013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fefc6e190b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 20,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH Model - Initialize with TruncatedNormal with kernel_regularizer=l2(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=30)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l2(0.1),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.1),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.1),\n",
    "                 kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.1),\n",
    "                kernel_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.1),\n",
    "                kernel_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_tn_reg_0.1_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_tn_reg_0.1', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/10\n",
      "599995/599995 [==============================] - 271s 452us/step - loss: 0.3520 - acc: 0.9568 - val_loss: 0.7647 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76343, saving model to Model_1_4_tn_reg_0.1_best.h5\n",
      "Epoch 2/10\n",
      "599995/599995 [==============================] - 278s 464us/step - loss: 0.2871 - acc: 0.9633 - val_loss: 0.7043 - val_acc: 0.7918\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76343 to 0.79178, saving model to Model_1_4_tn_reg_0.1_best.h5\n",
      "Epoch 3/10\n",
      "599995/599995 [==============================] - 276s 459us/step - loss: 0.2527 - acc: 0.9666 - val_loss: 0.6918 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79178 to 0.83651, saving model to Model_1_4_tn_reg_0.1_best.h5\n",
      "Epoch 4/10\n",
      "599995/599995 [==============================] - 275s 458us/step - loss: 0.2294 - acc: 0.9693 - val_loss: 0.5227 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.83651 to 0.86092, saving model to Model_1_4_tn_reg_0.1_best.h5\n",
      "Epoch 5/10\n",
      "599995/599995 [==============================] - 278s 464us/step - loss: 0.2142 - acc: 0.9708 - val_loss: 0.6197 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86092\n",
      "Epoch 6/10\n",
      "599995/599995 [==============================] - 278s 463us/step - loss: 0.2017 - acc: 0.9722 - val_loss: 0.9020 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86092\n",
      "Epoch 7/10\n",
      "599995/599995 [==============================] - 279s 465us/step - loss: 0.1918 - acc: 0.9735 - val_loss: 0.7871 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86092\n",
      "Epoch 8/10\n",
      "599995/599995 [==============================] - 277s 462us/step - loss: 0.1836 - acc: 0.9744 - val_loss: 0.5715 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86092 to 0.86713, saving model to Model_1_4_tn_reg_0.1_best.h5\n",
      "Epoch 9/10\n",
      "599995/599995 [==============================] - 280s 466us/step - loss: 0.1777 - acc: 0.9748 - val_loss: 1.1647 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86713\n",
      "Epoch 10/10\n",
      "599936/599995 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9757"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH Model - Initialize with TruncatedNormal for Kernel and Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=30)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_tn_bias_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_tn_bias', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599991 samples, validate on 600000 samples\n",
      "Epoch 1/15\n",
      "599991/599991 [==============================] - 266s 444us/step - loss: 1.0021 - acc: 0.9404 - val_loss: 0.4780 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92648, saving model to Model_1_4_tn_bias_best.h5\n",
      "Epoch 2/15\n",
      "599991/599991 [==============================] - 257s 428us/step - loss: 0.2546 - acc: 0.9722 - val_loss: 0.6479 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92648\n",
      "Epoch 3/15\n",
      "599991/599991 [==============================] - 256s 427us/step - loss: 0.1804 - acc: 0.9780 - val_loss: 0.5058 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92648\n",
      "Epoch 4/15\n",
      "599991/599991 [==============================] - 258s 430us/step - loss: 0.1483 - acc: 0.9806 - val_loss: 0.4641 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92648\n",
      "Epoch 5/15\n",
      "599991/599991 [==============================] - 260s 433us/step - loss: 0.1303 - acc: 0.9824 - val_loss: 0.4152 - val_acc: 0.8968\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92648\n",
      "Epoch 6/15\n",
      "599991/599991 [==============================] - 266s 443us/step - loss: 0.1180 - acc: 0.9837 - val_loss: 0.5779 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92648\n",
      "Epoch 7/15\n",
      "599991/599991 [==============================] - 260s 434us/step - loss: 0.1091 - acc: 0.9846 - val_loss: 0.7139 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92648\n",
      "Epoch 8/15\n",
      "599991/599991 [==============================] - 265s 442us/step - loss: 0.1031 - acc: 0.9856 - val_loss: 0.5691 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92648\n",
      "Epoch 9/15\n",
      "599991/599991 [==============================] - 261s 436us/step - loss: 0.0980 - acc: 0.9860 - val_loss: 0.8322 - val_acc: 0.7457\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92648\n",
      "Epoch 10/15\n",
      "599991/599991 [==============================] - 262s 437us/step - loss: 0.0936 - acc: 0.9866 - val_loss: 1.1281 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92648\n",
      "Epoch 11/15\n",
      "599991/599991 [==============================] - 262s 436us/step - loss: 0.0894 - acc: 0.9873 - val_loss: 0.6066 - val_acc: 0.8369\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92648\n",
      "Epoch 12/15\n",
      "599991/599991 [==============================] - 278s 463us/step - loss: 0.0865 - acc: 0.9877 - val_loss: 0.7988 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92648\n",
      "Epoch 13/15\n",
      "599991/599991 [==============================] - 275s 459us/step - loss: 0.0842 - acc: 0.9880 - val_loss: 0.4514 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92648\n",
      "Epoch 14/15\n",
      "599991/599991 [==============================] - 271s 452us/step - loss: 0.0815 - acc: 0.9884 - val_loss: 0.4777 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92648\n",
      "Epoch 15/15\n",
      "599991/599991 [==============================] - 281s 468us/step - loss: 0.0796 - acc: 0.9886 - val_loss: 0.7430 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8bbd2bdb70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 15,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UH Model - Initialize with TruncatedNormal for Kernel and Bias (different seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=100)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_tn_bias_seed_100_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_tn_bias_seed_100', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/10\n",
      "599995/599995 [==============================] - 285s 475us/step - loss: 0.9940 - acc: 0.9416 - val_loss: 0.6612 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84825, saving model to Model_1_4_tn_bias_seed_100_best.h5\n",
      "Epoch 2/10\n",
      "599995/599995 [==============================] - 275s 459us/step - loss: 0.2584 - acc: 0.9724 - val_loss: 0.6676 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.84825\n",
      "Epoch 3/10\n",
      "599995/599995 [==============================] - 279s 465us/step - loss: 0.1839 - acc: 0.9776 - val_loss: 0.5805 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.84825\n",
      "Epoch 4/10\n",
      "599995/599995 [==============================] - 290s 483us/step - loss: 0.1495 - acc: 0.9802 - val_loss: 0.5716 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84825 to 0.85699, saving model to Model_1_4_tn_bias_seed_100_best.h5\n",
      "Epoch 5/10\n",
      "599995/599995 [==============================] - 290s 484us/step - loss: 0.1301 - acc: 0.9820 - val_loss: 0.5853 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85699\n",
      "Epoch 6/10\n",
      "599995/599995 [==============================] - 279s 465us/step - loss: 0.1180 - acc: 0.9835 - val_loss: 0.5967 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85699\n",
      "Epoch 7/10\n",
      "599995/599995 [==============================] - 282s 469us/step - loss: 0.1094 - acc: 0.9845 - val_loss: 0.5583 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85699\n",
      "Epoch 8/10\n",
      "599995/599995 [==============================] - 286s 477us/step - loss: 0.1036 - acc: 0.9850 - val_loss: 0.6747 - val_acc: 0.8398\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85699\n",
      "Epoch 9/10\n",
      "599995/599995 [==============================] - 280s 467us/step - loss: 0.0977 - acc: 0.9858 - val_loss: 0.6900 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85699\n",
      "Epoch 10/10\n",
      "599995/599995 [==============================] - 292s 486us/step - loss: 0.0940 - acc: 0.9863 - val_loss: 0.5811 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f352d3d0a58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UH Model - Initialize with TruncatedNormal for Kernel and Bias L1-L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=100)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l1_l2(l1=0.01, l2=0.01),\n",
    "                 kernel_initializer=tn,\n",
    "                 bias_initializer=tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides= (1, 1), \n",
    "                 kernel_regularizer= l1_l2(l1=0.01, l2=0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer= l1_l2(l1=0.01, l2=0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer= l1_l2(l1=0.01, l2=0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer= l1_l2(l1=0.01, l2=0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_l1_l2_reg_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_l1_l2_reg', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/15\n",
      "599995/599995 [==============================] - 301s 502us/step - loss: 5.2209 - acc: 0.8809 - val_loss: 1.3691 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77445, saving model to Model_1_4_l1_l2_reg_best.h5\n",
      "Epoch 2/15\n",
      "599995/599995 [==============================] - 269s 449us/step - loss: 0.9547 - acc: 0.9227 - val_loss: 1.1357 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77445 to 0.82829, saving model to Model_1_4_l1_l2_reg_best.h5\n",
      "Epoch 3/15\n",
      "599995/599995 [==============================] - 273s 456us/step - loss: 0.8920 - acc: 0.9296 - val_loss: 1.2645 - val_acc: 0.7613\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82829\n",
      "Epoch 4/15\n",
      "599995/599995 [==============================] - 275s 458us/step - loss: 0.8652 - acc: 0.9327 - val_loss: 1.3898 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82829\n",
      "Epoch 5/15\n",
      "599995/599995 [==============================] - 301s 502us/step - loss: 0.8131 - acc: 0.9347 - val_loss: 2.2213 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82829\n",
      "Epoch 6/15\n",
      "599995/599995 [==============================] - 302s 504us/step - loss: 0.7914 - acc: 0.9376 - val_loss: 1.5676 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82829\n",
      "Epoch 7/15\n",
      "599995/599995 [==============================] - 302s 503us/step - loss: 0.8033 - acc: 0.9379 - val_loss: 1.2840 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82829\n",
      "Epoch 8/15\n",
      "599995/599995 [==============================] - 301s 502us/step - loss: 0.7955 - acc: 0.9408 - val_loss: 1.1780 - val_acc: 0.7818\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82829\n",
      "Epoch 9/15\n",
      "599995/599995 [==============================] - 301s 501us/step - loss: 0.7858 - acc: 0.9420 - val_loss: 1.2230 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82829\n",
      "Epoch 10/15\n",
      "599995/599995 [==============================] - 303s 505us/step - loss: 0.8085 - acc: 0.9419 - val_loss: 1.4579 - val_acc: 0.6706\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82829\n",
      "Epoch 11/15\n",
      "599995/599995 [==============================] - 302s 503us/step - loss: 0.8120 - acc: 0.9441 - val_loss: 1.2906 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82829\n",
      "Epoch 12/15\n",
      "599995/599995 [==============================] - 299s 498us/step - loss: 0.7989 - acc: 0.9443 - val_loss: 1.2916 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82829\n",
      "Epoch 13/15\n",
      "599995/599995 [==============================] - 303s 504us/step - loss: 0.8122 - acc: 0.9460 - val_loss: 1.3727 - val_acc: 0.7884\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82829\n",
      "Epoch 14/15\n",
      "599995/599995 [==============================] - 272s 453us/step - loss: 0.8085 - acc: 0.9454 - val_loss: 1.4106 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82829\n",
      "Epoch 15/15\n",
      "599995/599995 [==============================] - 270s 451us/step - loss: 0.7986 - acc: 0.9463 - val_loss: 1.5809 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f28481bf7f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 15,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UH Model - Initialize with TruncatedNormal for Kernel and Bias L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=100)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l1(0.01),\n",
    "                 kernel_initializer=tn,\n",
    "                 bias_initializer=tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides= (1, 1), \n",
    "                 kernel_regularizer= l1(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (3, 3), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer= l1(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer= l1(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer= l1(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_l1_reg_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_l1_reg', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/10\n",
      "599995/599995 [==============================] - 267s 445us/step - loss: 5.0421 - acc: 0.8808 - val_loss: 1.3247 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79976, saving model to Model_1_4_l1_reg_best.h5\n",
      "Epoch 2/10\n",
      "599995/599995 [==============================] - 295s 492us/step - loss: 0.9524 - acc: 0.9240 - val_loss: 1.2303 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.79976 to 0.85170, saving model to Model_1_4_l1_reg_best.h5\n",
      "Epoch 3/10\n",
      "599995/599995 [==============================] - 290s 483us/step - loss: 0.8735 - acc: 0.9300 - val_loss: 1.2304 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.85170\n",
      "Epoch 4/10\n",
      "599995/599995 [==============================] - 295s 491us/step - loss: 0.8254 - acc: 0.9333 - val_loss: 1.1869 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85170\n",
      "Epoch 5/10\n",
      "599995/599995 [==============================] - 287s 479us/step - loss: 0.8062 - acc: 0.9361 - val_loss: 1.2645 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85170\n",
      "Epoch 6/10\n",
      "599995/599995 [==============================] - 261s 435us/step - loss: 0.7793 - acc: 0.9373 - val_loss: 1.0992 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85170\n",
      "Epoch 7/10\n",
      "599995/599995 [==============================] - 259s 432us/step - loss: 0.7863 - acc: 0.9401 - val_loss: 1.3335 - val_acc: 0.7427\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85170\n",
      "Epoch 8/10\n",
      "599995/599995 [==============================] - 265s 442us/step - loss: 0.7903 - acc: 0.9421 - val_loss: 1.0578 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.85170 to 0.85432, saving model to Model_1_4_l1_reg_best.h5\n",
      "Epoch 9/10\n",
      "599995/599995 [==============================] - 259s 432us/step - loss: 0.7895 - acc: 0.9431 - val_loss: 1.7188 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85432\n",
      "Epoch 10/10\n",
      "599995/599995 [==============================] - 261s 435us/step - loss: 0.7623 - acc: 0.9440 - val_loss: 1.7062 - val_acc: 0.6580\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f28481bf128>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UH Model - Initialize with TruncatedNormal for Kernel and Bias (Kernel_Size = (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=30)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, \n",
    "                 kernel_size = (5, 5), \n",
    "                 strides=(1, 1), \n",
    "                 input_shape=(33, 33, 16), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (5, 5), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, \n",
    "                 kernel_size = (5, 5), \n",
    "                 strides=(1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_tn_bias_kernel_size_5_5_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_tn_bias_kernel_size_5_5', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599995 samples, validate on 600000 samples\n",
      "Epoch 1/10\n",
      "599995/599995 [==============================] - 247s 411us/step - loss: 1.1253 - acc: 0.9390 - val_loss: 0.9488 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80156, saving model to Model_1_4_tn_bias_kernel_size_5_5_best.h5\n",
      "Epoch 2/10\n",
      "599995/599995 [==============================] - 245s 408us/step - loss: 0.2733 - acc: 0.9692 - val_loss: 0.6221 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.80156 to 0.84412, saving model to Model_1_4_tn_bias_kernel_size_5_5_best.h5\n",
      "Epoch 3/10\n",
      "599995/599995 [==============================] - 253s 422us/step - loss: 0.1901 - acc: 0.9749 - val_loss: 0.7210 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.84412\n",
      "Epoch 4/10\n",
      "599995/599995 [==============================] - 247s 412us/step - loss: 0.1578 - acc: 0.9779 - val_loss: 0.8610 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.84412\n",
      "Epoch 5/10\n",
      "599995/599995 [==============================] - 257s 429us/step - loss: 0.1402 - acc: 0.9796 - val_loss: 0.5645 - val_acc: 0.8357\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.84412\n",
      "Epoch 6/10\n",
      "599995/599995 [==============================] - 254s 423us/step - loss: 0.1289 - acc: 0.9812 - val_loss: 0.6262 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.84412 to 0.86095, saving model to Model_1_4_tn_bias_kernel_size_5_5_best.h5\n",
      "Epoch 7/10\n",
      "599995/599995 [==============================] - 254s 423us/step - loss: 0.1207 - acc: 0.9823 - val_loss: 0.4688 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.86095 to 0.89245, saving model to Model_1_4_tn_bias_kernel_size_5_5_best.h5\n",
      "Epoch 8/10\n",
      "599995/599995 [==============================] - 234s 391us/step - loss: 0.1141 - acc: 0.9829 - val_loss: 0.7902 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89245\n",
      "Epoch 9/10\n",
      "599995/599995 [==============================] - 269s 448us/step - loss: 0.1089 - acc: 0.9837 - val_loss: 0.7901 - val_acc: 0.7743\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89245\n",
      "Epoch 10/10\n",
      "599995/599995 [==============================] - 243s 405us/step - loss: 0.1054 - acc: 0.9842 - val_loss: 0.4518 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26fc32e588>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D filter  Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:,:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test[:,:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((599996, 33, 33, 16, 1), (600000, 33, 33, 16, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tn = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=30)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(filters = 32, \n",
    "                 kernel_size = (3, 3, 3), \n",
    "                 strides=(1, 1, 1), \n",
    "                 input_shape=(33, 33, 16, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2),strides=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(filters = 64, \n",
    "                 kernel_size = (3, 3, 3), \n",
    "                 strides=(1, 1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Conv3D(filters = 64, \n",
    "                 kernel_size = (3, 3, 3), \n",
    "                 strides=(1, 1, 1), \n",
    "                 kernel_regularizer=l2(0.01),\n",
    "                 kernel_initializer= tn,\n",
    "                 bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2),strides=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                kernel_initializer= tn,\n",
    "                bias_initializer= tn))\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=1e-07, decay=0.0),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"Model_1_4_3D_kernel_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs_Model_1_4_3D_kernel', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint,tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599996 samples, validate on 600000 samples\n",
      "Epoch 1/10\n",
      "599996/599996 [==============================] - 1478s 2ms/step - loss: 1.2278 - acc: 0.9541 - val_loss: 1.3492 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69129, saving model to Model_1_4_3D_kernel_best.h5\n",
      "Epoch 2/10\n",
      "599996/599996 [==============================] - 1415s 2ms/step - loss: 0.2289 - acc: 0.9815 - val_loss: 1.4614 - val_acc: 0.6117\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69129\n",
      "Epoch 3/10\n",
      "599996/599996 [==============================] - 1415s 2ms/step - loss: 0.1524 - acc: 0.9850 - val_loss: 0.7107 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.69129 to 0.79146, saving model to Model_1_4_3D_kernel_best.h5\n",
      "Epoch 4/10\n",
      "599996/599996 [==============================] - 1415s 2ms/step - loss: 0.1234 - acc: 0.9866 - val_loss: 1.3573 - val_acc: 0.6611\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79146\n",
      "Epoch 5/10\n",
      "302208/599996 [==============>...............] - ETA: 8:33 - loss: 0.1101 - acc: 0.9875"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          batch_size= 128,\n",
    "          validation_data=(X_test, Y_test), callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
